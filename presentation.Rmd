---
title: "NYC Taxi"
author: "Rohan Danis-Cox"
date: "23/11/2019"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(feather)
```


```{r eval = FALSE, include= FALSE}
train <- read_feather("split_data/train.feather")
```

## Setting Up

Before I launch into the questions I need to prepare the data a little to make sure that I have a suitable base to conduct my analysis. This is a very large data set with over 15 million observations, but it looks to be just a single month of data, from April 2013.


```{r eval=FALSE}
  trip_data_raw <- read_csv("trip_data_4.csv")
  trip_fare_raw <- read_csv("trip_fare_4.csv")
  names(trip_data_raw)
  names(trip_fare_raw)
```

### Joining the data

The data comes across two spreadsheets but can be matched by joining on medallion,hack_license, vendor_id and pickup_datetime. Joining introduces a negligible increase in observations of about 1300 duplicates, mostly resulting from incorrect location data and payment types. 

```{r eval=FALSE}
  data <- left_join(trip_data_raw,trip_fare_raw, Joining, by = c("medallion", "hack_license", "vendor_id", "pickup_datetime"))
```

### Initial data preparation

There is a bit of fairly obvious filtering which should be done to remove observations which have:

* 0 passengers
* short trip times - here I have used less than 30 seconds
* short trip distances - say less than 100 meters
* pickup or dropoff longitude less than -75 or greater than -73
* pickup or dropoff latitude greater than 42 or less than 40

Furthermore, I can produce a few useful features like speed and day of week. I am going to assume the trip distance variable is in miles and therefore calculate speed in miles per hour. I am also assuming that average speed should be below 70 miles per hour.

```{r eval=FALSE}
  cleaning_1 <- data %>%
    filter(passenger_count != 0) %>%
    filter(trip_time_in_secs > 30) %>%
    filter(trip_distance > 0.1 ) %>%
    filter(pickup_longitude > -75 & pickup_longitude < -73) %>%
    filter(dropoff_longitude > -75 & dropoff_longitude < -73) %>%
    filter(pickup_latitude > 40 & pickup_latitude < 42) %>%
    filter(dropoff_latitude > 40 & dropoff_latitude < 42)

  cleaning_2 <- cleaning_1 %>%
    mutate(avg_speed_mh = trip_distance/trip_time_in_secs * 60 * 60) %>%
    filter(avg_speed_mh < 70)
  
  cleaning_3 <- cleaning_2 %>%
    mutate(pickup_hour = floor_date(pickup_datetime,"hour")) %>%
    mutate(day_of_week = wday(pickup_hour, label = TRUE))
```

This removes about 500,000 observations but leaves us with still well over 14 million. 

### Splitting the data

Even though I am working from a Google Cloud Platform instance the size of the data is inhibiting my ability to move quickly. One solution to this, which will be useful down the line when modelling is to split the data into a train, validation and test set. Not only does splitting before any analysis maintain the integrity of the test set, but it also allows me to work with a smaller, more manageable dataset.

Given this, I've decided to go with a 50/25/25 split using sample twice to achieve the necessary splits

```{r eval=FALSE}
  set.seed(11)
  index <- sample(1:nrow(cleaning_3), nrow(cleaning_3)/2)
  
  train <- cleaning_3[index,]
  remainder <- cleaning_3[-index,]
  
  set.seed(41)
  remainder_index <- sample(1:nrow(remainder), nrow(remainder)/2)
  val <- remainder[remainder_index,]
  test <- remainder[-remainder_index,]
```

I would need the buckets to be larger to be able to classify trips appropriately. 

```{r eval=FALSE}
  distinct_locations <- readRDS("saved_visualisations/distinct_locations.rds")
  distinct_locations
```

I've located some taxi zones on the [Taxi and Limousine commision site](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) and have tried to match this with the data to identify both pickup and drop off locations that could suit the 'trip' idea better.

Finally I'd want to 

### Splitting the data

Even though I am working from a Google Cloud Platform instance the size of the data is inhibiting my ability to move quickly. One solution to this, which will be useful down the line when modelling is to split the data into a train, validation and test set. Not only does splitting before any analysis maintain the integrity of the test set, but it also allows me to work with a smaller, more manageable dataset.

Given this, I've decided to go with a 50/25/25 split which I have achieved using the caret package to first split in half and then split the validation and test half again.

## Basic Questions

### a. What is the distribution of number of passengers per trip? 

```{r eval = FALSE}
  summary(train$passenger_count)
  
  ggplot(train, aes(passenger_count)) +
    geom_bar() + 
    scale_x_continuous(breaks = c(1:6)) + 
    ggtitle("Distribution of Passengers") +
    theme_minimal()
```


### b. What is the distribution of payment_type? 

```{r eval = FALSE}

```


### c. What is the distribution of fare amount? 

```{r eval = FALSE}

```


### d. What is the distribution of tip amount? 

```{r eval = FALSE}

```


### e. What is the distribution of total amount? 

```{r eval = FALSE}

```


### f. What are top 5 busiest hours of the day? 

```{r eval = FALSE}

```


### g. What are the top 10 busiest locations of the city? 

I've located some taxi zones on the [Taxi and Limousine commision site](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) and have tried to match this with the data to identify both pickup and drop off locations that could suit the 'trip' idea better.

### h. Which trip has the highest standard deviation of travel time? 

### i. Which trip has most consistent fares? 

## Open Questions

### a. In what trips can you confidently use respective means as measures of central tendency to estimate fare, time taken, etc. 

### b. Can we build a model to predict fare and tip amount given pick up and drop off coordinates, time of day and week? 

### c. If you were a taxi owner, how would you maximize your earnings in a day? 

### d. If you were a taxi owner, how would you minimize your work time while retaining the average wages earned by a typical taxi in the dataset? 

### e. If you run a taxi company with 10 taxis, how would you maximize your earnings? 


