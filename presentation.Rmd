---
title: "NYC Taxi"
author: "Rohan Danis-Cox"
date: "23/11/2019"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(feather)
library(dplyr)
library(scales)
library(sf)
library(stringr)
library(tidyr)
library(purrr)
library(broom)
library(randomForest)
```


```{r eval = TRUE, include= FALSE}
train <- read_feather("split_data/train.feather")
train_2 <- read_feather("split_data/train_2.feather")
val <- read_feather("split_data/val.feather")
taxi_zone_busy <- readRDS("taxi_zone_busy.rds")
all_hacks <- read_feather("all_hacks.feather")
taxi_zones <- read_sf("taxi_zones") %>%
    st_transform(4326)
top_hack <- read_feather("top_hack.feather")
top_10_hacks <- read_feather("top_10_hacks.feather")
typical_taxi <- read_feather("typical_taxi.feather")
location_time <- read_feather("location_time.feather")
day_hour <- read_feather("day_hour.feather")
```

## Setting Up

Before I launch into the questions I need to prepare the data a little to make sure that I have a suitable base to conduct my analysis. This is a very large data set with over 15 million observations, but it looks to be just a single month of data, from April 2013.


```{r eval=FALSE}
  trip_data_raw <- read_csv("trip_data_4.csv")
  trip_fare_raw <- read_csv("trip_fare_4.csv")
  names(trip_data_raw)
  names(trip_fare_raw)
```

### Joining the data

The data comes across two spreadsheets but can be matched by joining on medallion,hack_license, vendor_id and pickup_datetime. Joining introduces a negligible increase in observations of about 1300 duplicates, mostly resulting from incorrect location data and payment types. I am expecting these to be stripped out when I clean variables next.

```{r eval=FALSE}
  data <- left_join(trip_data_raw,trip_fare_raw, Joining, by = c("medallion", "hack_license", "vendor_id", "pickup_datetime"))
```

### Initial data preparation

There is a bit of fairly obvious filtering which should be done to remove observations which have:

* 0 passengers
* short trip times - say less than 30 seconds
* short trip distances - say less than 100 meters
* pickup or dropoff longitude less than -75 or greater than -73 
* pickup or dropoff latitude greater than 42 or less than 40

Furthermore, I can produce a few useful features like speed and day of week. I am going to assume the trip distance variable is in miles and therefore calculate speed in miles per hour. I am also assuming that average speed should be below 70 miles per hour.

```{r eval=FALSE}
  cleaning_1 <- data %>%
    filter(passenger_count != 0) %>%
    filter(trip_time_in_secs > 30) %>%
    filter(trip_distance > 0.1 ) %>%
    filter(pickup_longitude > -75 & pickup_longitude < -73) %>%
    filter(dropoff_longitude > -75 & dropoff_longitude < -73) %>%
    filter(pickup_latitude > 40 & pickup_latitude < 42) %>%
    filter(dropoff_latitude > 40 & dropoff_latitude < 42)

  cleaning_2 <- cleaning_1 %>%
    mutate(avg_speed_mh = trip_distance/trip_time_in_secs * 60 * 60) %>%
    filter(avg_speed_mh < 70)
  
  cleaning_3 <- cleaning_2 %>%
    mutate(pickup_hour = floor_date(pickup_datetime,"hour")) %>%
    mutate(day_of_week = wday(pickup_hour, label = TRUE))
```

This removes about 500,000 observations leaving well over 14 million. 

### Splitting the data

Even though I am working from a Google Cloud Platform instance the size of the data is inhibiting my ability to move quickly. One solution to this, which will be useful down the line when modelling is to split the data into a train, validation and test set. Not only does splitting before any analysis maintain the integrity of the test set, but it also allows me to work with a smaller, more manageable dataset.

Given this, I've decided to go with a 50/25/25 split using sample twice to achieve the necessary splits

```{r eval=FALSE}
  set.seed(11)
  index <- sample(1:nrow(cleaning_3), nrow(cleaning_3)/2)
  
  train <- cleaning_3[index,]
  remainder <- cleaning_3[-index,]
  
  set.seed(41)
  remainder_index <- sample(1:nrow(remainder), nrow(remainder)/2)
  val <- remainder[remainder_index,]
  test <- remainder[-remainder_index,]
```

There may still be a need for cleaning variables but it might be best to look at this after addressing some of the basic questions

## Basic Questions

### a. What is the distribution of number of passengers per trip? 

The number of passengers per trip is a discrete value and has a mean of `r train %>% summarise(mean = mean(passenger_count)) %>% pull() %>% round(2)`. As this is count data which has a zero lower bound and the mean is close to 1, we would expect to see something like a poisson distibution which is highly right-skewed with a mode of 1 down to the maximum value of 6. This is precisely what we see in the below plot.

```{r eval = TRUE}
  ggplot(train, aes(passenger_count)) +
    geom_bar() + 
    scale_x_continuous(breaks = c(1:6)) + 
    scale_y_continuous(labels = comma) +
    ggtitle("Distribution of Passengers") +
    labs(x = "Number of Passengers") +
    theme_minimal()
```

### b. What is the distribution of payment_type? 

The below plot shows the frequency associated with each payment type and clearly indicates a bimodal structure within the categorical variable. Paying by cash or card represent the two options, with a variable small number of disputed, not charged or unknown payments. 

```{r eval = TRUE}
  ggplot(train, aes(payment_type)) +
      geom_bar() + 
      ggtitle("Distribution of Payment Type") +
      labs(x = "Payment Method", y = "Frequency") +
      theme_minimal()
```

Interestingly, I will show below that tips are only really recorded when the passenger pays with card. Cash tips are obviously just pocketed by the driver.

### c. What is the distribution of fare amount? 

Fare amount is a right-skewed distribution with a median of `r train %>% summarise(median(fare_amount)) %>% pull() %>% round(2)` and a mean of `r train %>% summarise(mean(fare_amount)) %>% pull() %>% round(2)`. The most expensive fares are over $400 but most fares are below $50. Strangely, there is a large spike at exactly $52 which may suggest situations where the drive does not go by the meter and instead has a fixed cost for a certain route.

```{r eval = TRUE}
  ggplot(train, aes(fare_amount)) +
      geom_histogram(bins = 500) + 
      scale_y_continuous(labels = comma) +
      ggtitle("Distribution of Fare Amount") +
      labs(x = "Fare Amount", y = "Frequency") +
      theme_minimal()
```

Stealing a bit of thunder from my future investigations I am able to classify where these routes are and confirm that `r train_2 %>% select(fare_amount,pickup_zone) %>% filter(fare_amount == 52) %>% count(pickup_zone) %>% arrange(desc(n)) %>% head(1) %>% select(n) %>% pull()` are picked up from JFK Airport and `r train_2 %>% select(fare_amount,dropoff_zone) %>% filter(fare_amount == 52) %>% count(dropoff_zone) %>% arrange(desc(n)) %>% head(1) %>% select(n) %>% pull()` are being dropped off at JFK airport. As shown below, these passengers are mainly being taken to and from Manhattan.

```{r eval = TRUE}
  pickup_JFK <- train_2 %>%
    select(fare_amount,pickup_zone,dropoff_borough) %>%
    filter(fare_amount == 52) %>%
    filter(pickup_zone == "JFK Airport") %>%
    count(dropoff_borough) %>%
    arrange(desc(n))

  pickup_JFK

  dropoff_JFK <- train_2 %>%
    select(fare_amount,dropoff_zone,pickup_borough) %>%
    filter(fare_amount == 52) %>%
    filter(dropoff_zone == "JFK Airport") %>%
    count(pickup_borough) %>%
    arrange(desc(n))
  
  dropoff_JFK
```

### d. What is the distribution of tip amount? 
The below plot shows the distribution of tip amount as predominatly 0 and then tailing off despite some large outlier tip values. 

```{r eval = TRUE}
  ggplot(train,aes(tip_amount)) +
      geom_histogram(bins = 500) + 
      ggtitle("Distribution of Tip Amount") +
      labs(x = "Tip Amount", y = "Frequency") +
      theme_minimal()
```

This plot isn't very clear so I have zoomed in on the data to show tips up to $20 and colour coded this based on the payment method. It is clear from this plot that drivers are more than likely receiving tips but just not recording them when the payment is with cash. 

```{r eval = TRUE, echo = FALSE}
  ggplot(train,aes(tip_amount, fill = payment_type)) +
    geom_histogram(bins = 500) + 
    coord_cartesian(xlim = c(0,20),ylim=c(0, 800000))+
    scale_x_continuous(breaks = c(1:20)) +
    scale_y_continuous(labels = comma) +
    ggtitle("Only Card Tips are Recorded") +
    labs(x = "Tip Amount", y = "Frequency") +
    theme_minimal()
```

The distribution of only card payments, where we can more reliably consider the tip amount resembles a poisson distribution around a mean of `r train %>% select(tip_amount,payment_type) %>% filter(payment_type == "CRD")%>% summarise(mean = mean(tip_amount)) %>% pull() %>% round(2)`.

```{r eval = TRUE, echo = FALSE}
card_tip <- train %>%
    select(tip_amount,payment_type) %>%
    filter(payment_type == "CRD")

ggplot(card_tip, aes(tip_amount)) +
    geom_histogram(bins = 500) + 
    coord_cartesian(xlim = c(0,20)) +
    ggtitle("Distribution of Tip Amount When Paid With Card") +
    labs(x = "Tip Amount") +
    theme_minimal()
```

### e. What is the distribution of total amount? 

The distribution of total amount looks much like that of fare amount, that is a heavily right-skewed distributon, although the spike at $52 has been washed out somewhat by tips and taxes.
```{r eval = TRUE, echo = FALSE}
  ggplot(train, aes(total_amount)) +
    geom_histogram(bins = 500) + 
    scale_y_continuous(labels = comma) +
    ggtitle("Distribution of Total Amount") +
    labs(x = "Total Amount", y = "Frequency") +
    theme_minimal()
```

### f. What are top 5 busiest hours of the day? 

As one might expect, the busiest hours for taxis are in the evening, where between 6-11pm where there is a noticeable platform compared with the rest of the day. 

```{r eval = TRUE}
  busy_hours <- train %>%
    group_by(pickup_hour) %>%
    summarise(count= n()) %>%
    mutate(rank = rank(desc(count))) %>%
    mutate(top_5 = case_when(rank <= 5 ~ "Yes",
                             TRUE ~ "No"))
```

```{r eval = TRUE, echo = FALSE}
  ggplot(busy_hours,aes(x = pickup_hour,y= count, fill = top_5)) +
    geom_bar(stat = "identity") +
    scale_fill_manual(values=c("grey","red")) +
    scale_y_continuous(labels = comma) +
    ggtitle("Top 5 Busiest Hours - 6-11pm") +
    labs(fill = "Top 5", x = "Pickup Hour", y = "Frequency") + 
    theme_minimal()
```

It would be interesting to see how working in the period of time performs as one might expect it the market to be saturated with relatively short low yield trips as people travel to meet engagements.

### g. What are the top 10 busiest locations of the city? 

As touched on earlier, with this question in mind I sourced some additional spatial data to help make sense of the location coordinates provided in the data set. The below plot shows why I feel this was necessary, as each coordinate was relatively distinct and so made it very challenging to group the data. 

```{r eval = TRUE, echo = FALSE, warning= FALSE}
  distinct_pickup <- train %>% distinct(pickup_longitude, pickup_latitude) 
  sample_index <- sample(1:nrow(distinct_pickup), nrow(distinct_pickup)/100)
  distinct_sample <- distinct_pickup[sample_index,]

    ggplot(distinct_sample, aes(x = pickup_longitude, y = pickup_latitude)) + 
      geom_point(alpha = 0.3, size = 0.01) + 
      xlim(-74.05,-73.85) + 
      ylim(40.65,40.85) + 
      ggtitle("Coordinates of Pickups") +
      labs(x = "Longitude", y = "Latitude") + 
      theme_minimal()
```

I could have rounded the coordinates to generate a grid system, and this is what I would have attempted if I wasn't able to locate a shapefile which served a similar purpose but provided more meaning to the data.

Fortunately I found a great shapefile of taxi zones at the [Taxi and Limousine commision site](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page) and I was able to join the data based on the simple features themselves - see investigations file for how this was done. 

```{r eval = TRUE}
  busy_pickup_locations <- train_2 %>%
    group_by(pickup_zone) %>%
    summarise(pickup= n()) 
  
  busy_dropoff_locations <- train_2 %>%
    group_by(dropoff_zone) %>%
    summarise(dropoff= n())
  
  taxi_zones_to_borough <- taxi_zone_busy %>%
    select(zone,borough) %>%
    st_drop_geometry()
  
  busy_overall <- busy_pickup_locations %>%
    left_join(busy_dropoff_locations, by = c("pickup_zone" = "dropoff_zone")) %>%
    mutate(overall = pickup + dropoff) %>%
    left_join(taxi_zones_to_borough, by = c("pickup_zone" = "zone")) %>%
    select(zone = pickup_zone, borough, overall) %>%
    mutate(rank = rank(desc(overall))) %>%
    mutate(top_10 = case_when(rank <= 10 ~ "Yes",
                             TRUE ~ "No"))
  
  busy_overall %>% filter(top_10 == "Yes") %>% arrange (desc(overall)) %>% select(zone, borough,trips = overall)
```

The above table shows that the busiest locations, as measured by taxi zones of pickups and dropoffs are all in Manhattan. 

Visualising the data on a map clearly shows Manhattan's prominence but also indicates a large number of trips to the airports at La Guardia and JFK.

```{r eval = TRUE}
  ggplot(taxi_zone_busy) + 
    geom_sf(mapping = aes(fill = overall, colour = borough)) +
    scale_fill_gradient(low = "white", high = "red", label = comma) + 
    ggtitle("Taxi Zone Pickups") +
    labs(x = "Longitude", y = "Latitude", fill = "Number of Pickups", colour = "Borough") + 
    theme(panel.grid.major = element_line(color = gray(0.5), linetype = "dashed", size = 0.1))
```

### h. Which trip has the highest standard deviation of travel time? 

Originally I wanted to classify this using a trip consisting of a zones and hours. The reason I wanted to do this is that travel time could be expected to vary greatly based on traffic (which was probably the actual intent of the question). But when I did this JFK to JFK showed up pretty unanimously at all hours. This is probably fair since a some trips would have been a few hundred meters down the road and surely others would have been half-way across the city and back to retrieve important items. 

```{r eval = TRUE}
  high_sd_time <- train_2 %>%
    group_by(pickup_zone,dropoff_zone) %>%
    summarise(st_dev_time = sd(trip_time_in_secs),
              average_distance = mean(trip_distance),
              n = n()) %>%
    filter(!is.na(pickup_zone)) %>%
    filter(!is.na(dropoff_zone)) %>%
    filter(n > 20) %>% 
    arrange(desc(st_dev_time)) %>%
    ungroup() %>%
    head(10)
  
  high_sd_time
```

So instead I am looking independent of time and looking at trips with at least 20 observations. Going from Masbeth to JFK tops the lot with a standard deviation of over `high_sd_time %>% head(1) %>% mutate(x = round(st_dev_time,0)/60) %>% select(x) %>% pull()` minutes. These trips are on average `r train_2 %>% filter(pickup_zone == "Maspeth" & dropoff_zone == "JFK Airport") %>% summarise(mean = round(mean(trip_distance),2)) %>% pull()` miles and so this seems reasonable. The below map shows these two trips to give you an idae of the distances involved.

```{r eval = TRUE, echo = FALSE}
high_sd_map <- taxi_zone_busy %>%
      select(zone) %>%
      mutate(high_sd = case_when(zone %in% c("Maspeth", "JFK Airport") ~ "Maspeth  to JFK",
                                 zone %in% c("South Jamaica", "Midtown South") ~ "South Jamaica  to Midtown South",
                                 TRUE ~ ""))

  ggplot(high_sd_map) + 
      geom_sf(mapping = aes(fill = high_sd)) + 
      scale_fill_manual(values = c("white","red", "blue")) +
      ggtitle("Trips with High Standard Deviation of Time") +
      labs(x = "Longitude", y = "Latitude", fill = "Trip") + 
      coord_sf(xlim = c(-73.7,-74.05),ylim=c(40.55,40.85))+
      theme(panel.grid.major = element_line(color = gray(0.5), linetype = "dashed", size = 0.1))
```

### i. Which trip has most consistent fares? 

Taking a similar approach to the previous question shows two short journeys going from Cobble Hill to Colombia Street in Brooklyn and from Seaport to Battery Park in Manhattan as having the most consistent overall fare. In the case of the all the top 10 the standard deviation is showing that the majority of fares are within $1.70 of the average total fare. 

Again, this makes sense as they are predominatenly stort trips, mostly below 2 miles.

```{r eval = TRUE}
  low_sd_fare <- train_2 %>%
    group_by(pickup_zone,dropoff_zone) %>%
    summarise(st_dev_fare = sd(total_amount), 
              average_distance = mean(trip_distance),
              n = n()) %>%
    filter(!is.na(pickup_zone)) %>%
    filter(!is.na(dropoff_zone)) %>%
    filter(n > 20) %>% 
    arrange(st_dev_fare) %>%
    ungroup() %>%
    head(10)
  
  high_sd_time
```

```{r eval = TRUE, echo = FALSE}
  low_sd_map <- taxi_zone_busy %>%
    select(zone) %>%
    mutate(low_sd = case_when(zone %in% c("Cobble Hill", "Columbia Street") ~ "Cobble Hill to Colombia Street",
                               zone %in% c("Seaport", "Battery Park") ~ "Seaport to Battery Park",
                               TRUE ~ ""))

  ggplot(low_sd_map) + 
      geom_sf(mapping = aes(fill = low_sd)) + 
      scale_fill_manual(values = c("white","red", "blue")) +
      ggtitle("Trips with Low Standard Deviation in Fare") +
      labs(x = "Longitude", y = "Latitude", fill = "Trip") + 
      coord_sf(xlim = c(-73.95,-74.05),ylim=c(40.66,40.76))+
      theme(panel.grid.major = element_line(color = gray(0.5), linetype = "dashed", size = 0.1))
```


## Open Questions

### a. In what trips can you confidently use respective means as measures of central tendency to estimate fare, time taken, etc. 

Mean can be used as a measure of central tendancy when the data is symetrical, i.e. not exihibiting signs of skewness. As established above, fare is heavily right-skewed when viewing the data as a whole, but when looking at particular trips it could be expected that you would see distributions which are more normally distributed. 

We can look at parametric statistical tests to understand how a sample distribution compares to a normal distribution, with one example being the Shapiro-Wilk’s test. A statisitc close to 1 suggests normal distribution but it is best to view the histogram or qplot to confirm.

Just a quick aside, when applying the Shapiro-Wilk test in R the sample size is limited to between 3 and 5000. This is primarily because the null hypothesis is that the distibution is normal (which is odd for an hypothesis test as usually it would be the other way around) and very small samples tend to confirm the null while very large sample sizes tend to reject the null.

Let's take a look at the fare over trips: 

```{r eval = TRUE, echo = FALSE, warning = FALSE}
  trips <- train_2 %>%
      filter(!is.na(pickup_zone) & !is.na(dropoff_zone)) %>%
      mutate(trip = paste0(pickup_zone," to ", dropoff_zone))
  
  shap_fare <- trips %>%
      select(trip,fare_amount) %>%
      group_by(trip) %>%
      mutate(row = row_number(),
             n = n(),
             min = min(fare_amount),
             max = max(fare_amount),
             dif = min - max) %>%
      filter(row < 5000 & n>30 & dif != 0) %>%
      ungroup() %>%
      select(-c(row,n,min,max,dif)) %>%
      nest(-trip) %>% 
      mutate(shapiro = map(data, ~ shapiro.test(.x$fare_amount)),
             tidied = map(shapiro,tidy)) %>%
      unnest(tidied) %>%
      select(-c(data,shapiro))
  
  shap_fare_1 <- shap_fare %>%
    top_n(10,statistic)
  
  shap_fare_1
```

The above table shows the top 10 trips based on the Shapiro-Wilk test. Examining one of these examples, we would expect to see data which is normally distributed.

```{r eval = TRUE, echo = FALSE}
  example_1 <- trips %>%
    filter(trip == "Midtown North to Seaport")
  
  ggplot(example_1,aes(fare_amount)) +
    geom_histogram(bins = 50) +
    ggtitle("Distribution of Fare Amount from Midtown North to Seaport") +
    labs(x = "Fare Amount", y = "Frequency") +
    theme_minimal()
```

```{r eval = TRUE, echo = TRUE}
  shap_fare_2 <- shap_fare %>%
    filter(!str_detect(trip,"JFK")) %>%
    top_n(-10,statistic)
  
  shap_fare_2
```

On the other hand, this table shows some low scoring trips (I've excluded trips involving JFK as the already established $52 fares create some very strange distributions), and we would expect to see data that is not normally distributed. These would be cases where you would probably not want to use the mean as a central measure of tendancy. Such at the below, where the mean value would be raised away from the mode and median by a few significant outliers.

```{r eval = TRUE, echo = FALSE}
  example_2 <- trips %>%
    filter(trip == "Brooklyn Heights to Cobble Hill")
  
  ggplot(example_2,aes(fare_amount)) +
    geom_histogram(bins = 50) +
    ggtitle("Distribution of Fare Amount from Brooklyn Heights to Cobble Hill") +
    labs(x = "Fare Amount", y = "Frequency") +
    theme_minimal()
```

The same can be done for trip time:

```{r eval = TRUE, echo = FALSE}
  shap_time <- trips %>%
    select(trip,trip_time_in_secs) %>%
    group_by(trip) %>%
    mutate(row = row_number(),
           n = n(),
           min = min(trip_time_in_secs),
           max = max(trip_time_in_secs),
           dif = min - max) %>%
    filter(row < 5000 & n>30 & dif != 0) %>%
    ungroup() %>%
    select(-c(row,n,min,max,dif)) %>%
    nest(-trip) %>% 
    mutate(shapiro = map(data, ~ shapiro.test(.x$trip_time_in_secs)),
           tidied = map(shapiro,tidy)) %>%
    unnest(tidied) %>%
    select(-c(data,shapiro))

  shap_time_1 <- shap_time %>%
    top_n(10,statistic)
  
  shap_time_1
```

And we would expect a similar result:

```{r eval = TRUE, echo = FALSE}
  example_3 <- trips %>%
      filter(trip == "Financial District North to Central Park")
      
    ggplot(example_3,aes(trip_time_in_secs)) +
      geom_histogram(bins = 50) +
      ggtitle("Distribution of Trip Time from Financial District North to Central Park") +
      labs(x = "Fare Amount", y = "Frequency") +
      theme_minimal()
```

### b. Can we build a model to predict fare and tip amount given pick up and drop off coordinates, time of day and day of week? 

Thinking logically, we should be able to predict fares with reasonable accuracy. My belief for this stems from the prescibed nature of fares where we could expect variables like distance travelled, length of trip, number of passengers, time of day, tolls etc to be taken into account in determining a fare. Whether we can accurately predict the fare from only location, time of day and DAY of week is less clear (I am assuming the question meant to say day of week as this would seem to me to make more sense).

In terms of tip amount, we have already established that the data only captures tip amounts when they are paid by card. Even if I were to isolate for this, I wouldn't expect a model to be very effective in estimating tip amounts as it depends on many more human variables which are very challenging to measure. How generous is the passenger? How friendly was the driver? How urgent was the trip? All variables that are going to be very challenging to measure.

Below I have demonstrated how a random forest using only 5,000 observations from the training set can do an okay job at predicting fare amount. To aid the algorithm I have simplified time of day into 5 periods.

```{r eval = TRUE, echo = FALSE}
  
  train_1 <- train %>%
    mutate(pickup_time = as.factor(case_when(pickup_hour >= 2 & pickup_hour < 7 ~ "Early Morning",
                                   pickup_hour >= 7 & pickup_hour < 12 ~ "Morning",
                                   pickup_hour >= 12 & pickup_hour < 18 ~ "Afternoon",
                                   pickup_hour >= 18 & pickup_hour < 22 ~ "Evening",
                                   pickup_hour >= 22 | pickup_hour < 2 ~ "Late Night")))

val_1 <- val %>%
    mutate(pickup_time = as.factor(case_when(pickup_hour >= 2 & pickup_hour < 7 ~ "Early Morning",
                                   pickup_hour >= 7 & pickup_hour < 12 ~ "Morning",
                                   pickup_hour >= 12 & pickup_hour < 18 ~ "Afternoon",
                                   pickup_hour >= 18 & pickup_hour < 22 ~ "Evening",
                                   pickup_hour >= 22 | pickup_hour < 2 ~ "Late Night")))

  set.seed(16)
    rf_sample_index <- sample(1:nrow(train_1), 5000)
    rf_sample <- train_1[rf_sample_index,]
    
    rf_x <- rf_sample[,c(11:14,24:25)]
    rf_y <- rf_sample$fare_amount
    
    rf_1 <- randomForest(y = rf_y, x = rf_x)
    
  # Compared to only top 100000 values from validation set
    
  val_check <- val_1 %>%
    head(100000) %>%
    select(11:14,24,25,16)
  
  val_pred <- val_check %>%
    mutate(pred = predict(object = rf_1,newdata = val_check))
  
  val_pred_rf <- val_check %>%
    mutate(pred = predict(object = rf_1,newdata = val_check))
  
  rf_rmse <- val_pred_rf %>%
    summarise(rmse = sqrt(mean((pred - fare_amount)^2)))
  
  head(val_pred)
  rf_rmse
```

This model achieves an root mean squared error (RMSE) of `r rf_rmse %>% pull()`. This suggests that on average the predicted values are about `r rf_rmse %>% pull() %>% round(2)` dollars away from the actual value of fare amount. The below plot of actual vs predicted values shows that this model performs relatively well.

```{r eval = TRUE, echo = FALSE}
  ggplot(val_pred_rf,aes(x = fare_amount,pred)) + 
      geom_point(alpha = 0.1, size = 0.3) +
      coord_cartesian(xlim = c(0,60),ylim=c(0, 60)) +
      ggtitle("Actual vs Predicted - Fare Amount") +
      labs(x = "Fare Amount", y = "Predicted Fare Amount") +
      theme_minimal()
```

Taking aim at tip amount the below model shows that RMSE alone may not tell the whole story.

```{r eval = TRUE, echo = FALSE}
  rf_y_2 <- rf_sample$tip_amount
  
  rf_2 <- randomForest(y = rf_y_2, x = rf_x)
  
  val_check_2 <- val_1 %>%
    head(100000) %>%
    select(11:14,24,25,19)
  
  val_pred_rf_2 <- val_check_2 %>%
    mutate(pred = predict(object = rf_2,newdata = val_check))
  
  rf_rmse_2 <- val_pred_rf_2 %>%
    summarise(rmse = sqrt(mean((pred - tip_amount)^2)))
```

This model achieves an RMSE of `r rf_rmse_2 %>% pull()`. This suggests that on average the predicted values are about `r rf_rmse_2 %>% pull() %>% round(2)` dollars away from the actual value of tip amount. However, the below plot of actual vs predicted values shows that the model tends to underestimate tip amount. I'd suspect that this was on account of the 0 values coming from cash transactions.

```{r eval = TRUE, echo = FALSE}
  ggplot(val_pred_rf_2,aes(x = tip_amount,pred)) + 
      geom_point(alpha = 0.1, size = 0.3) +
      coord_cartesian(xlim = c(0,20),ylim=c(0, 20)) +
      ggtitle("Actual vs Predicted - Tip Amount") +
      labs(x = "Tip Amount", y = "Predicted Fare Amount") +
      theme_minimal()
```

### c. If you were a taxi owner, how would you maximize your earnings in a day? 

I am going to approach the next two questions in different ways - the first requires identifying top performing drivers aka hacks and then looking specifically at their data to understand what they do differently. The second is to look purely at the things which I can focus on, time and location.

After grouping the full data set by hack license (not by medallian as it appears as though some drivers work across several cabs) I am able to compute some summary statistics, such as:

- total trips
- total working time in hours
- total miles travelled
- total fares
- total tips
- earnings per day
- earnings per trip
- earnings per hour
- earnings per mile
- earnings per hour and per mile
- petrol costs, assumed to be 28 miles per gallon and a cost of $3.32 per gallon

I then want to filter out outliers which is mainly achieved by doing some common sense stuff, i.e. Must have worked at least 10 days, at least 4 hours per day, done over 100 trips and travelled at least 50 miles.

The objective here is to maximise earnings in a day, so I think a good place to start is who is maximising earnings per hour after petrol is account for. 

```{r eval = TRUE, echo = FALSE}

  all_hacks_1 <- all_hacks %>%
    filter(total_days_working > 10 &
             total_trips > 100 & 
             total_miles_travelled > 50 & 
             avg_hours_per_day > 4) %>%
    arrange(desc(avg_hourly_earnings_after_petrol))  
  
  top_10 <- all_hacks_1 %>%
    head(10)
  
  top_10
```

After doing that I have the top 10 drivers, who are all earning over $90 per hour. Take a bow `r top_10 %>% head(1) %>% select(hack_license) %>% pull()`.

So what is this driver doing that is so amazing? Well they are primarily waiting at La Guardia to take people into Manhattan.

```{r eval = TRUE, echo = FALSE}
  top_hack_1 <- taxi_zones %>%
    left_join(top_hack %>% count(pickup_zone), by = c("zone" = "pickup_zone")) %>%
    mutate(n = case_when(is.na(n) ~ as.integer(0),
                         TRUE ~ n))
  
  top_hack_2 <- taxi_zones %>%
    left_join(top_hack %>% count(dropoff_zone), by = c("zone" = "dropoff_zone")) %>%
    mutate(n = case_when(is.na(n) ~ as.integer(0),
                         TRUE ~ n))
  
  ggplot(top_hack_1) + 
    geom_sf(mapping = aes(fill = n)) +
    scale_fill_gradient(low = "white", high = "red") + 
    ggtitle("Top Hack Pickups") +
    labs(x = "Longitude", y = "Latitude", fill = "Number of Pickups") + 
    xlim(-74.05,-73.75) + 
    ylim(40.6,40.9) + 
    theme(panel.grid.major = element_line(color = gray(0.5), linetype = "dashed", size = 0.1))
  
  ggplot(top_hack_2) + 
    geom_sf(mapping = aes(fill = n)) +
    scale_fill_gradient(low = "white", high = "red") + 
    ggtitle("Top Hack Dropoffs") +
    labs(x = "Longitude", y = "Latitude", fill = "Number of Dropoffs") + 
    xlim(-74.05,-73.75) + 
    ylim(40.6,40.9) + 
    theme(panel.grid.major = element_line(color = gray(0.5), linetype = "dashed", size = 0.1)) 
  
```

What about the other guys in the top 10, are they all at La Guardia? No, in fact as you can see below the preferred lurking spot is actually at JFK.

```{r eval = TRUE, echo = FALSE}
  top_10_hacks_1 <- taxi_zones %>%
    left_join(top_10_hacks %>% count(pickup_zone), by = c("zone" = "pickup_zone")) %>%
    mutate(n = case_when(is.na(n) ~ as.integer(0),
                         TRUE ~ n))
  
  top_10_hacks_2 <- taxi_zones %>%
    left_join(top_10_hacks %>% count(dropoff_zone), by = c("zone" = "dropoff_zone")) %>%
    mutate(n = case_when(is.na(n) ~ as.integer(0),
                         TRUE ~ n))
  
  ggplot(top_10_hacks_1) + 
    geom_sf(mapping = aes(fill = n)) +
    scale_fill_gradient(low = "white", high = "red") + 
    ggtitle("Top 10 Hacks Pickups") +
    labs(x = "Longitude", y = "Latitude", fill = "Number of Pickups") + 
    theme(panel.grid.major = element_line(color = gray(0.5), linetype = "dashed", size = 0.1))
  
  ggplot(top_10_hacks_2) + 
    geom_sf(mapping = aes(fill = n)) +
    scale_fill_gradient(low = "white", high = "red") + 
    ggtitle("Top 10 Hacks Dropoffs") +
    labs(x = "Longitude", y = "Latitude", fill = "Number of Dropoffs") + 
    theme(panel.grid.major = element_line(color = gray(0.5), linetype = "dashed", size = 0.1)) 
  
```

What about when it comes to hours? Well the top guy does exclusively late nights and the top 10 seem to prefer that as well.

```{r eval = TRUE, echo = FALSE}
  ggplot(top_hack, aes(x = day_of_week, y = pickup_hour)) + 
      geom_point(alpha = 0.3, colour = "red") + 
      ggtitle("Top Hack - Works Late Nights") +
      labs(x = "Day of Week", y = "Pickup Hour", subtitle = "Day doesn't seem to matter, but this person is a night owl") + 
      theme_minimal()

  ggplot(top_10_hacks, aes(x = day_of_week, y = pickup_hour)) + 
    geom_jitter(alpha = 0.1, colour = "red") + 
    ggtitle("Top 10 Hacks - Mostly Work Nights") +
    labs(x = "Day of Week", y = "Pickup Hour") + 
    theme_minimal()
```

Regardless, what these guys are doing is definitely working as they are well above the pack. Based on this, to maximise my earnings I would base my pickup strategy entirely around working out of La Guardia or JFK and aim to work late evenings to early mornings.

```{r eval = TRUE, echo = FALSE}
ggplot(all_hacks_1,aes(x = total_hours_working, y = avg_hourly_earnings_after_petrol)) + 
    geom_point(alpha = 0.1, size = 0.2) + 
    geom_point(data = top_10, aes(x = total_hours_working, y = avg_hourly_earnings_after_petrol), colour = "red", size = 0.7) +
    ggtitle("Total Hours Working vs Hourly Earnings After Petrol - Top Hacks in Red") +
    labs(x = "Total Hours Working", y = "Hourly Earnings After Petrol", subtitle = "Focusing on the Airports at Night Works") + 
    theme_minimal()
  
```

<b>Post-work note: I've noticed two key assumption that will most likely void the above. I've assumed hourly work to be based on the sum of trip times which excludes the important time spent looking for a fare. An improved analysis would require looking at the sequence of dropoff and pickup times to understand periods of work.</b>

### d. If you were a taxi owner, how would you minimize your work time while retaining the average wages earned by a typical taxi in the dataset? 

```{r eval = TRUE, echo = FALSE}

medians <- all_hacks %>%
    summarise(median_trips = median(total_trips),
              median_total_hours = median(total_hours_working),
              median_miles_travelled = median(total_miles_travelled),
              median_total_fare = median(total_fare),
              median_total_tips = median(total_tips),
              median_monthly_take = median(total_take),
              median_paid_for_petrol = median(total_paid_for_petrol),
              median_earnings = median(total_earnings),
              median_days_working = median(total_days_working),
              average_medallians_used = mean(total_mediallians_used), # gives me an idea of the outliers, median is 1
              median_hours_per_day = median(avg_hours_per_day),
              median_earnings_per_day = median(avg_earnings_per_day),
              median_earnings_per_trip = median(avg_earnings_per_trip),
              median_earnings_per_hour = median(avg_earnings_per_hour),
              median_earnings_per_mile = median(avg_earnings_per_mile),
              median_earnings_per_hour_per_mile = median(avg_earnings_per_hour_and_per_mile),
              median_hourly_earnings_after_petrol = median(avg_hourly_earnings_after_petrol))
  location_time_1 <- location_time %>%
    filter(total_trips > 100) %>%
    filter(!is.na(pickup_zone))
  
  hours_required <- location_time_1 %>%
    mutate(quartile = ntile(avg_earnings_per_hour,4),
           hours_required_to_meet_month_earnings = medians$median_earnings/avg_earnings_per_hour)
```

For this question I am taking an entirely different approach, not looking at drivers but instead looking at the things a driver can influence, where they look for fares, on what days and at what times.  

The median monthly earnings for a driver in this data is $`r medians %>% select(median_earnings) %>% pull() %>% round(0)`. Looking at the various pickup locations, days and times the top quartile (assuming I could find it) would have an average of `r hours_required %>% filter(quartile == 4) %>% summarise(mean(hours_required_to_meet_month_earnings)) %>% pull() %>% round(0)` hours to hit this average monthly earnings. Assuming I like the idea of weekends this works out to a bit over 3 hours per day.

So ignoring locations for now, what day and hours should I be working? Basically I want to be starting after midnight and packing in before 6am.

```{r eval = TRUE, echo = FALSE}
  ggplot(day_hour,aes(x = pickup_hour,y = avg_earnings_per_hour, colour = day_of_week)) +
      geom_line() + 
      ggtitle("Hourly Earnings By Hour and Day") +
      labs(x = "Hour", y = "Hourly Earnings", subtitle = "The best bet are the hours between midnight and 5 on weekdays") + 
      scale_color_discrete() +
      theme_minimal()
```

So with that in mind, I've generated a map which could sit in my dashboard window telling me where I should be:
```{r eval = TRUE, echo = FALSE}
location_time_1 <- location_time %>%
    filter(total_trips > 100) %>%
    filter(!is.na(pickup_zone)) %>%
    filter(!day_of_week %in% c("Sat","Sun")) %>%
    filter(pickup_hour < 6) %>%
    group_by(pickup_zone) %>%
    summarise(mean_hourly_earnings = mean(avg_earnings_per_hour))
  
  map_guide <- taxi_zones %>%
    left_join(location_time_1, by = c("zone" = "pickup_zone"))
  
  ggplot(map_guide) + 
    geom_sf(mapping = aes(fill = mean_hourly_earnings)) +
    scale_fill_gradient2(low = "white", high = "red", na.value = "white") + 
    ggtitle("Working in these zones early weekday mornings...") +
    labs(x = "Longitude", y = "Latitude", fill = "Average Hourly Earnings", subtitle = "I can earn the median wage in only 90 hours or 4 hours per day") + 
    theme(panel.grid.major = element_line(color = gray(0.5), linetype = "dashed", size = 0.1)) 
```

### e. If you run a taxi company with 10 taxis, how would you maximize your earnings? 

Running with a similar theme, for this question I will look at medallions rather than drivers and rank them into deciles by total earnings after petrol. Following the KISS methodology of keeping things simple I've then looked for the taxi zones which have the highest average decile score and identified the following:

```{r eval = TRUE, echo = FALSE}
company_plan <- cleaning_3 %>%
    group_by(medallion) %>%
    mutate(total_miles_travelled = sum(trip_distance),
              total_take = sum(total_amount),
              total_paid_for_petrol = total_miles_travelled/28*3.32,
              total_earnings = total_take - total_paid_for_petrol) %>%
    ungroup() %>%
    mutate(earnings_decile = ntile(total_earnings,10)) %>%
    group_by(pickup_borough,pickup_zone,day_of_week,pickup_hour) %>%
    summarise(n = n(),
              competing_taxi = n_distinct(medallion),
              average_earnings_decile = mean(earnings_decile)) 
  
  company_plan_1 <- company_plan %>%
    filter(n >1000)
  
  company_plan_2 <- company_plan_1 %>%
    group_by(pickup_zone,pickup_borough) %>%
    summarise(average_earnings_decile = mean(average_earnings_decile)) %>%
    arrange(desc(average_earnings_decile)) %>%
    head(10)
  
  company_plan_2
```

The final instructions to my fleet of drivers is work the below map until you get tired then come in and sub out. Since the fares are out there and the cost of petrol and cars is relatively inexpensive, it makes sense to just go hell-for-leather the whole time.

```{r eval = TRUE, echo = FALSE}

  company_guide <- taxi_zones %>%
    left_join(company_plan_2, by = c("zone" = "pickup_zone"))
  
  ggplot(company_guide) + 
    geom_sf(mapping = aes(fill = average_earnings_decile)) +
    scale_fill_gradient2(low = "white", high = "red", na.value = "white") + 
    xlim(-74.05,-73.85) + 
    ylim(40.65,40.85) + 
    ggtitle("Run 24/7 from these locations") +
    labs(x = "Longitude", y = "Latitude", fill = "Average Earnings Decile", subtitle = "This is where medallions are maximising their value") + 
    theme(panel.grid.major = element_line(color = gray(0.5), linetype = "dashed", size = 0.1))
```
